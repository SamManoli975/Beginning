{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict emotion percentages\n",
    "def predict_emotion(sentence):\n",
    "    vectorized_text = vectorizer(tf.constant([sentence]))\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    emotion_percentages = prediction[0] * 100  # Convert to percentage\n",
    "    outstanding_emotion_index = np.argmax(prediction[0])\n",
    "    outstanding_emotion = emotion_labels[outstanding_emotion_index]\n",
    "    return emotion_percentages, outstanding_emotion\n",
    "\n",
    "# Example prediction\n",
    "sentence = \"I feel really happy today!\"\n",
    "percentages, top_emotion = predict_emotion(sentence)\n",
    "print(f\"Emotion Percentages: {dict(zip(emotion_labels, percentages))}\")\n",
    "print(f\"Outstanding Emotion: {top_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('training.csv')  # Replace with your dataset path\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0                            i didnt feel humiliated      0\n",
      "1  i can go from feeling so hopeless to so damned...      0\n",
      "2   im grabbing a minute to post i feel greedy wrong      3\n",
      "3  i am ever feeling nostalgic about the fireplac...      2\n",
      "4                               i am feeling grouchy      3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('training.csv')  # Replace with your dataset path\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']  # Sentences\n",
    "y = df['label']  # Emotion labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['Anger', 'Fear', 'Joy', 'Love', 'Sadness', 'Surprise']\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(emotion_labels)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = label_binarizer.transform([emotion_labels[i] for i in y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "max_features = 10000  # Vocabulary size\n",
    "sequence_length = 70  # Maximum sequence length\n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_sequence_length=sequence_length,\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on training data\n",
    "vectorizer.adapt(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer(X_train)\n",
    "X_test_vectorized = vectorizer(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input, Embedding, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "num_labels = y_one_hot.shape[1]\n",
    "\n",
    "input_layer = Input(shape=(sequence_length,), dtype=tf.int32, name='text_input')\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=128)(input_layer)\n",
    "x = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "x = MaxPooling1D(pool_size=4)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output_layer = Dense(num_labels, activation='sigmoid')(x)  # Sigmoid for multi-label classification\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=BinaryCrossentropy(), metrics=[Accuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.5048 - val_accuracy: 0.0000e+00 - val_loss: 0.4112\n",
      "Epoch 2/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.4056 - val_accuracy: 0.0000e+00 - val_loss: 0.4099\n",
      "Epoch 3/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.4054 - val_accuracy: 0.0000e+00 - val_loss: 0.4094\n",
      "Epoch 4/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.4010 - val_accuracy: 0.0000e+00 - val_loss: 0.4059\n",
      "Epoch 5/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 0.3953 - val_accuracy: 0.0000e+00 - val_loss: 0.3934\n",
      "Epoch 6/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.3680 - val_accuracy: 0.0000e+00 - val_loss: 0.3428\n",
      "Epoch 7/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 0.3009 - val_accuracy: 0.0000e+00 - val_loss: 0.2804\n",
      "Epoch 8/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 0.2361 - val_accuracy: 0.0000e+00 - val_loss: 0.2389\n",
      "Epoch 9/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.1918 - val_accuracy: 0.0000e+00 - val_loss: 0.2177\n",
      "Epoch 10/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.1684 - val_accuracy: 0.0000e+00 - val_loss: 0.2056\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_vectorized,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 4.4665e-05 - loss: 0.2041\n",
      "Test Loss: 0.2034\n",
      "Test Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_vectorized, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Emotion Percentages: {'Anger': 7.4262724, 'Fear': 52.642464, 'Joy': 6.430655, 'Love': 7.9768195, 'Sadness': 0.2590005, 'Surprise': 2.651053}\n",
      "Outstanding Emotion: Fear\n"
     ]
    }
   ],
   "source": [
    "# Function to predict emotion percentages\n",
    "def predict_emotion(sentence):\n",
    "    vectorized_text = vectorizer(tf.constant([sentence]))\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    emotion_percentages = prediction[0] * 100  # Convert to percentage\n",
    "    outstanding_emotion_index = np.argmax(prediction[0])\n",
    "    outstanding_emotion = emotion_labels[outstanding_emotion_index]\n",
    "    return emotion_percentages, outstanding_emotion\n",
    "\n",
    "# Example prediction\n",
    "sentence = \"I feel really happy today!\"\n",
    "percentages, top_emotion = predict_emotion(sentence)\n",
    "print(f\"Emotion Percentages: {dict(zip(emotion_labels, percentages))}\")\n",
    "print(f\"Outstanding Emotion: {top_emotion}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
